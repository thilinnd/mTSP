{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8996e561",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES & DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578161e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src') \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from GA import solve\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e81af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLGA_mTSP:\n",
    "    def __init__(self, distance_matrix: np.ndarray, m: int = 3,\n",
    "                 n_population: int = 100, max_iterations: int = 1000,\n",
    "                 epsilon: float = 0.1, epsilon_decay: float = 0.995,\n",
    "                 n_states: int = 10):\n",
    "        # --- Thông tin đầu vào và khởi tạo tham số ---\n",
    "        self.distance_matrix = distance_matrix\n",
    "        self.n_cities = len(distance_matrix)\n",
    "        self.m = m\n",
    "        self.N = n_population\n",
    "        self.Max = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # --- Không gian hành động RL ---\n",
    "        self.Cs = np.linspace(0.1, 0.9, 9)  # Crossover rates\n",
    "        self.Ms = np.linspace(0.01, 0.1, 10)  # Mutation rates\n",
    "        self.S = list(range(n_states))  # Dummy state set (không dùng trực tiếp)\n",
    "        self.Ns = n_states\n",
    "        self.Nt = 10  # Ngưỡng số hành động để chia pha học RL\n",
    "\n",
    "        # --- Q-table và lịch sử trạng thái ---\n",
    "        self.Q = {}\n",
    "        self.state_history = []\n",
    "        self.initialize_q_table()  # Đặt sẵn hàm tạo Q-table động\n",
    "\n",
    "        # --- Tham số hiện tại ---\n",
    "        self.current_state = 0\n",
    "        self.stagnation_counter = 0\n",
    "        self.Pc = random.choice(self.Cs)\n",
    "        self.Pm = random.choice(self.Ms)\n",
    "\n",
    "        # --- Khởi tạo quần thể ---\n",
    "        self.population = self._initialize_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gọi local search\n",
    "def local_search(self, route):\n",
    "    best = route[:]\n",
    "    best_routes = self.tsp_split_dp(best)\n",
    "    best_fitness = max(self.calculate_route_distance(r) for r in best_routes)\n",
    "\n",
    "    for _ in range(10):\n",
    "        i, j = random.sample(range(len(route)), 2)\n",
    "        new = best[:]\n",
    "        new[i], new[j] = new[j], new[i]\n",
    "        new_routes = self.tsp_split_dp(new)\n",
    "        new_fitness = max(self.calculate_route_distance(r) for r in new_routes)\n",
    "        if new_fitness < best_fitness:\n",
    "            best = new\n",
    "            best_fitness = new_fitness\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7bc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _initialize_population(self) -> List[List[int]]:\n",
    "        # Khởi tạo quần thể các cá thể ngẫu nhiên (không bao gồm depot)\n",
    "        population = []\n",
    "        for _ in range(self.N):\n",
    "            route = list(range(1, self.n_cities))\n",
    "            random.shuffle(route)\n",
    "            population.append(route)\n",
    "        return population\n",
    "\n",
    "    def calculate_route_distance(self, route: List[int]) -> float:\n",
    "        # Tính tổng quãng đường của tuyến đi qua các thành phố\n",
    "        return sum(self.distance_matrix[route[i]][route[i + 1]] for i in range(len(route) - 1))\n",
    "\n",
    "    def tsp_split_dp(self, route: List[int]) -> List[List[int]]:\n",
    "        # --- Tách tuyến bằng dynamic programming ---\n",
    "        n = len(route)\n",
    "        dp = [[float('inf')] * (self.m + 1) for _ in range(n + 1)]\n",
    "        path = [[-1] * (self.m + 1) for _ in range(n + 1)]\n",
    "        dp[0][0] = 0\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            for k in range(1, self.m + 1):\n",
    "                for j in range(i):\n",
    "                    sub_route = [0] + route[j:i] + [0]\n",
    "                    cost = self.calculate_route_distance(sub_route)\n",
    "                    if max(dp[j][k - 1], cost) < dp[i][k]:\n",
    "                        dp[i][k] = max(dp[j][k - 1], cost)\n",
    "                        path[i][k] = j\n",
    "\n",
    "        # --- Truy vết tuyến tối ưu ---\n",
    "        routes = []\n",
    "        i, k = n, self.m\n",
    "        while k > 0:\n",
    "            j = path[i][k]\n",
    "            routes.append([0] + route[j:i] + [0])\n",
    "            i, k = j, k - 1\n",
    "\n",
    "        return routes[::-1]\n",
    "\n",
    "    def calculate_fitness(self, individual: List[int]) -> Tuple[float, float, List[List[int]]]:\n",
    "        # Tính fitness của cá thể: max_distance, total_distance, các tuyến\n",
    "        routes = self.tsp_split_dp(individual)\n",
    "        distances = [self.calculate_route_distance(route) for route in routes]\n",
    "        return max(distances), sum(distances), routes\n",
    "\n",
    "    def initialize_q_table(self):\n",
    "        pass  # Q-table sẽ khởi tạo khi gặp state mới\n",
    "\n",
    "    def get_state(self, fitness_scores: List[Tuple[float, float]], iteration: int) -> str:\n",
    "        # --- Biểu diễn trạng thái theo đa yếu tố (diversity, improvement, convergence) ---\n",
    "        max_distances = [f[0] for f in fitness_scores]\n",
    "        diversity = np.std(max_distances)\n",
    "        improvement_rate = 0\n",
    "\n",
    "        if len(self.state_history) > 0:\n",
    "            prev_best = min(self.state_history[-1])\n",
    "            current_best = min(max_distances)\n",
    "            improvement_rate = (prev_best - current_best) / (prev_best + 1e-6)\n",
    "\n",
    "        diversity_level = min(int(diversity * 10), 9)\n",
    "        improvement_level = max(0, min(int(improvement_rate * 100), 9))\n",
    "        convergence_level = min(int(iteration / self.Max * 10), 9)\n",
    "\n",
    "        state = f\"div_{diversity_level}_imp_{improvement_level}_conv_{convergence_level}\"\n",
    "        self.state_history.append(max_distances)\n",
    "\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = {}\n",
    "            for pc in self.Cs:\n",
    "                for pm in self.Ms:\n",
    "                    self.Q[state][(pc, pm)] = 0.0\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262e9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def select_action_epsilon_greedy(self, state: str) -> Tuple[float, float]:\n",
    "        # --- Chính sách chọn hành động epsilon-greedy ---\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.Cs), random.choice(self.Ms)\n",
    "        else:\n",
    "            return max(self.Q[state].items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    def update_q_value(self, state: str, action: Tuple[float, float], reward: float, next_state: str, iteration: int):\n",
    "        # --- Cập nhật Q-value theo Q-learning ---\n",
    "        alpha, gamma = 0.1, 0.9\n",
    "        current_q = self.Q[state][action]\n",
    "        next_max_q = max(self.Q[next_state].values()) if next_state in self.Q else 0\n",
    "\n",
    "        if (self.Nt - self.Ns) / 2 > iteration:\n",
    "            self.Q[state][action] = current_q + alpha * (reward - current_q)\n",
    "        else:\n",
    "            self.Q[state][action] = current_q + alpha * (reward + gamma * next_max_q - current_q)\n",
    "\n",
    "    def tournament_selection(self, population: List[List[int]], fitness_scores: List[Tuple[float, float]], k: int = 3) -> List[int]:\n",
    "        # --- Chọn lọc theo giải đấu ---\n",
    "        indices = random.sample(range(len(population)), k)\n",
    "        winner_idx = min(indices, key=lambda i: fitness_scores[i][0])\n",
    "        return population[winner_idx]\n",
    "\n",
    "    def crossover(self, parent1: List[int], parent2: List[int]) -> Tuple[List[int], List[int]]:\n",
    "        # --- Lai ghép OX ---\n",
    "        if random.random() > self.Pc:\n",
    "            return parent1.copy(), parent2.copy()\n",
    "\n",
    "        size = len(parent1)\n",
    "        a, b = sorted(random.sample(range(size), 2))\n",
    "\n",
    "        def ox(p1, p2):\n",
    "            child = [-1] * size\n",
    "            child[a:b + 1] = p1[a:b + 1]\n",
    "            fill = [x for x in p2 if x not in child]\n",
    "            j = 0\n",
    "            for i in range(size):\n",
    "                if child[i] == -1:\n",
    "                    child[i] = fill[j]\n",
    "                    j += 1\n",
    "            return child\n",
    "\n",
    "        return ox(parent1, parent2), ox(parent2, parent1)\n",
    "\n",
    "    def mutate(self, route: List[int]) -> List[int]:\n",
    "        # --- Đột biến hoán vị 2 điểm ---\n",
    "        route = route.copy()\n",
    "        if random.random() < self.Pm:\n",
    "            i, j = random.sample(range(len(route)), 2)\n",
    "            route[i], route[j] = route[j], route[i]\n",
    "        return route\n",
    "\n",
    "    def run(self) -> Tuple[List[List[int]], float, float]:\n",
    "        # --- Vòng lặp chính của RLGA ---\n",
    "        child1 = self.local_search(child1)\n",
    "        child2 = self.local_search(child2)\n",
    "\n",
    "        best_solution, best_max_distance, best_total_distance = None, float('inf'), float('inf')\n",
    "        t = 0\n",
    "\n",
    "        while t < self.Max:\n",
    "            # --- Đánh giá quần thể hiện tại ---\n",
    "            fitness_scores = [self.calculate_fitness(ind)[:2] for ind in self.population]\n",
    "            current_state = self.get_state(fitness_scores, t)\n",
    "            self.Pc, self.Pm = self.select_action_epsilon_greedy(current_state)\n",
    "\n",
    "            # --- Tạo quần thể mới ---\n",
    "            new_population = []\n",
    "            while len(new_population) < self.N:\n",
    "                p1 = self.tournament_selection(self.population, fitness_scores)\n",
    "                p2 = self.tournament_selection(self.population, fitness_scores)\n",
    "                c1, c2 = self.crossover(p1, p2)\n",
    "                new_population.extend([self.mutate(c1), self.mutate(c2)])\n",
    "\n",
    "            self.population = new_population[:self.N]\n",
    "            new_fitness_scores = [self.calculate_fitness(ind) for ind in self.population]\n",
    "\n",
    "            # --- Cập nhật lời giải tốt nhất ---\n",
    "            for i, (max_d, total_d, routes) in enumerate(new_fitness_scores):\n",
    "                if max_d < best_max_distance:\n",
    "                    best_max_distance, best_total_distance = max_d, total_d\n",
    "                    best_solution = routes\n",
    "\n",
    "            # --- Tính reward và cập nhật Q ---\n",
    "            old_best = min([f[0] for f in fitness_scores])\n",
    "            new_best = min([f[0] for f in new_fitness_scores])\n",
    "            reward = (old_best - new_best) / (old_best + 1e-6)\n",
    "\n",
    "            next_state = self.get_state([f[:2] for f in new_fitness_scores], t + 1)\n",
    "            self.update_q_value(current_state, (self.Pc, self.Pm), reward, next_state, t)\n",
    "\n",
    "            # --- Điều chỉnh epsilon ---\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            t += 1\n",
    "\n",
    "            # --- Ghi log ---\n",
    "            if t % 100 == 0:\n",
    "                print(f\"Iteration {t}: Best max distance = {best_max_distance:.2f}, Pc = {self.Pc:.2f}, Pm = {self.Pm:.2f}, state = {current_state}\")\n",
    "\n",
    "        return best_solution, best_max_distance, best_total_distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
